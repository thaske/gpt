{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91648b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(encoding_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696f8f4",
   "metadata": {},
   "source": [
    "# Input Text/Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac426482",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793faa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6a48bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.eot_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1789d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I HAD always thought<|endoftext|>',\n",
       " tensor([   40,   367,  2885,  1464,  1807, 50256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = raw_text[:20] + '<|endoftext|>'\n",
    "encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "inputs = torch.tensor(encoded)\n",
    "text, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48177b9c",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea6b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 3\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ff990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3966e+00, -9.9491e-01, -1.5822e-03],\n",
       "         [-1.1659e+00,  1.3834e-01, -9.8013e-01],\n",
       "         [ 1.0122e+00, -6.5152e-01,  8.6051e-02],\n",
       "         [ 2.5264e-01,  2.1043e+00, -6.4511e-01],\n",
       "         [-3.1843e-01, -1.2264e-01, -3.9307e-02],\n",
       "         [-3.1300e-01,  7.5582e-01, -1.2656e+00]], grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "token_embeddings, token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff10c5c",
   "metadata": {},
   "source": [
    "# Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f6e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = len(inputs)\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccea24e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1583,  1.0919,  0.8055],\n",
       "         [ 1.1262, -0.8857, -1.6315],\n",
       "         [-0.2570,  0.7127, -0.3622],\n",
       "         [ 0.9609, -1.3697,  0.1381],\n",
       "         [-1.2365,  1.9319,  0.4730],\n",
       "         [ 0.7365,  0.1316,  0.2379]], grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "pos_embeddings, pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e2cbb",
   "metadata": {},
   "source": [
    "# Add them up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beba89a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.5549,  0.0969,  0.8039],\n",
       "         [-0.0397, -0.7474, -2.6117],\n",
       "         [ 0.7552,  0.0611, -0.2762],\n",
       "         [ 1.2135,  0.7346, -0.5071],\n",
       "         [-1.5549,  1.8092,  0.4337],\n",
       "         [ 0.4235,  0.8874, -1.0277]], grad_fn=<AddBackward0>),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings, input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87ff8f",
   "metadata": {},
   "source": [
    "# Simple self-attention (no trainable weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d22328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5549,  0.0969,  0.8039],\n",
       "        [-0.0397, -0.7474, -2.6117],\n",
       "        [ 0.7552,  0.0611, -0.2762],\n",
       "        [ 1.2135,  0.7346, -0.5071],\n",
       "        [-1.5549,  1.8092,  0.4337],\n",
       "        [ 0.4235,  0.8874, -1.0277]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6810bae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0397, -0.7474, -2.6117], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input_embeddings[1] # Second input token will be our query\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd02643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941d6cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = torch.empty(input_embeddings.shape[0])\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79f87cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0397, -0.7474, -2.6117], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1170f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of tensor([1.5500, 0.1000, 0.8000]) and tensor([-0.0400, -0.7500, -2.6100]):\n",
      "1.55 * -0.04 + 0.10 * -0.75 + 0.80 * -2.61 = -2.23\n",
      "\n",
      "\n",
      "dot product of tensor([-0.0400, -0.7500, -2.6100]) and tensor([-0.0400, -0.7500, -2.6100]):\n",
      "-0.04 * -0.04 + -0.75 * -0.75 + -2.61 * -2.61 = 7.38\n",
      "\n",
      "\n",
      "dot product of tensor([ 0.7600,  0.0600, -0.2800]) and tensor([-0.0400, -0.7500, -2.6100]):\n",
      "0.76 * -0.04 + 0.06 * -0.75 + -0.28 * -2.61 = 0.65\n",
      "\n",
      "\n",
      "dot product of tensor([ 1.2100,  0.7300, -0.5100]) and tensor([-0.0400, -0.7500, -2.6100]):\n",
      "1.21 * -0.04 + 0.73 * -0.75 + -0.51 * -2.61 = 0.73\n",
      "\n",
      "\n",
      "dot product of tensor([-1.5500,  1.8100,  0.4300]) and tensor([-0.0400, -0.7500, -2.6100]):\n",
      "-1.55 * -0.04 + 1.81 * -0.75 + 0.43 * -2.61 = -2.42\n",
      "\n",
      "\n",
      "dot product of tensor([ 0.4200,  0.8900, -1.0300]) and tensor([-0.0400, -0.7500, -2.6100]):\n",
      "0.42 * -0.04 + 0.89 * -0.75 + -1.03 * -2.61 = 2.00\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.2338,  7.3810,  0.6456,  0.7270, -2.4230,  2.0040])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, x_i in enumerate(input_embeddings):\n",
    "  dp = torch.dot(x_i, query)\n",
    "  attn_scores_2[i] = dp\n",
    "  print(f\"dot product of {x_i.round(decimals=2).data} and {query.round(decimals=2).data}:\")\n",
    "  print(\" + \".join([f\"{x_i[idx]:.2f} * {element:.2f}\" for idx, element in enumerate(query)]), end=f\" = {dp:.2f}\")\n",
    "  print(\"\\n\\n\")\n",
    "attn_scores_2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edfaceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False, False],\n",
      "        [False,  True, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False,  True,  True, False, False],\n",
      "        [False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False,  True]])\n",
      "tensor([[3.0734, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, 7.3810, 0.0000, 0.0000, -0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1014, 2.2694, -0.0000, 0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, 5.8791, 0.0000],\n",
      "        [-0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0230]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = input_embeddings @ input_embeddings.T\n",
    "mask = attn_scores == attn_scores.max(dim=0, keepdim=True).values\n",
    "print(mask)\n",
    "\n",
    "attn_scores = attn_scores * mask\n",
    "print(attn_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
