{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91648b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(encoding_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696f8f4",
   "metadata": {},
   "source": [
    "# Input Text/Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac426482",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793faa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6a48bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.eot_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1789d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I HAD always thought<|endoftext|>',\n",
       " tensor([   40,   367,  2885,  1464,  1807, 50256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = raw_text[:20] + '<|endoftext|>'\n",
    "encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "inputs = torch.tensor(encoded)\n",
    "text, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48177b9c",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea6b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 3\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ff990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3966e+00, -9.9491e-01, -1.5822e-03],\n",
       "         [-1.1659e+00,  1.3834e-01, -9.8013e-01],\n",
       "         [ 1.0122e+00, -6.5152e-01,  8.6052e-02],\n",
       "         [ 2.5264e-01,  2.1043e+00, -6.4511e-01],\n",
       "         [-3.1843e-01, -1.2264e-01, -3.9307e-02],\n",
       "         [-3.1300e-01,  7.5582e-01, -1.2656e+00]], grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "token_embeddings, token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff10c5c",
   "metadata": {},
   "source": [
    "# Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f6e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = len(inputs)\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccea24e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1583,  1.0919,  0.8055],\n",
       "         [ 1.1262, -0.8857, -1.6315],\n",
       "         [-0.2570,  0.7127, -0.3622],\n",
       "         [ 0.9609, -1.3697,  0.1381],\n",
       "         [-1.2365,  1.9319,  0.4730],\n",
       "         [ 0.7365,  0.1316,  0.2379]], grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "pos_embeddings, pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e2cbb",
   "metadata": {},
   "source": [
    "# Add them up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beba89a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.5549,  0.0969,  0.8039],\n",
       "         [-0.0397, -0.7474, -2.6117],\n",
       "         [ 0.7552,  0.0611, -0.2762],\n",
       "         [ 1.2135,  0.7346, -0.5071],\n",
       "         [-1.5549,  1.8092,  0.4337],\n",
       "         [ 0.4235,  0.8874, -1.0277]], grad_fn=<AddBackward0>),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings, input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87ff8f",
   "metadata": {},
   "source": [
    "# Simple self-attention (no trainable weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d22328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your\n",
    "        [0.55, 0.87, 0.66], # journey\n",
    "        [0.57, 0.85, 0.64], # starts\n",
    "        [0.22, 0.58, 0.33], # with\n",
    "        [0.77, 0.25, 0.10], # one\n",
    "        [0.05, 0.80, 0.55], # step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6810bae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1] # Second input token will be our query\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd02643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941d6cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3049e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79f87cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1170f969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "  attn_scores_2[i] = torch.dot(x_i, query)\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "632c0546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=-1)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1daa898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0596, 0.0208, 0.1233]) = 0.14 * tensor([0.4300, 0.1500, 0.8900])\n",
      "tensor([0.1308, 0.2070, 0.1570]) = 0.24 * tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.1330, 0.1983, 0.1493]) = 0.23 * tensor([0.5700, 0.8500, 0.6400])\n",
      "tensor([0.0273, 0.0719, 0.0409]) = 0.12 * tensor([0.2200, 0.5800, 0.3300])\n",
      "tensor([0.0833, 0.0270, 0.0108]) = 0.11 * tensor([0.7700, 0.2500, 0.1000])\n",
      "tensor([0.0079, 0.1265, 0.0870]) = 0.16 * tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "query, query.shape, context_vec_2\n",
    "for i, x_i in enumerate(inputs):\n",
    "  context_vec_2 += attn_weights_2[i] * x_i\n",
    "  print(f\"{attn_weights_2[i] * x_i} = {attn_weights_2[i]:.2f} * {x_i}\")\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edfaceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False, False],\n",
      "        [False,  True,  True,  True, False,  True],\n",
      "        [False, False, False, False,  True, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False]])\n",
      "tensor([[0.9995, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4950, 1.4754, 0.8434, 0.0000, 1.0865],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7154, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "mask = attn_scores == attn_scores.max(dim=0, keepdim=True).values\n",
    "print(mask)\n",
    "\n",
    "attn_scores = attn_scores * mask\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f772995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83dab672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a2311b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "584e8d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca590da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43c2bb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5500, 0.8700, 0.6600]), 3, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "x_2, d_in, d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dfb2387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d35992e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4306, 1.4551]), tensor([0.4433, 1.1419]), tensor([0.3951, 1.0037]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "query_2, key_2, value_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3f8c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]]) tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "print(keys, query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e04bccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8524), tensor([0.4433, 1.1419]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "attn_scores_22, keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f8dc03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "440bf3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([[0.3669, 0.4433, 0.4361, 0.2408, 0.1827, 0.3275],\n",
      "        [0.7646, 1.1419, 1.1156, 0.6706, 0.3292, 0.9642]])\n"
     ]
    }
   ],
   "source": [
    "print(query_2)\n",
    "print(keys.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a95f0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551]) tensor([0.3669, 0.7646]) tensor(1.2705)\n"
     ]
    }
   ],
   "source": [
    "print(query_2, keys.T[:, 0], query_2 @ keys.T[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ebb0f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f5f66fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2  = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "767999c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dcd6cfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
