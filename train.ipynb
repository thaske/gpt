{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b7a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea923a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\").with_format(\"torch\")\n",
    "dataloader = DataLoader(ds.get(\"train\"), batch_size=1)\n",
    "enc = tiktoken.get_encoding(encoding_name=\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ce0e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5045,\n",
       " 2163,\n",
       " 11,\n",
       " 261,\n",
       " 3389,\n",
       " 8881,\n",
       " 11484,\n",
       " 72406,\n",
       " 2491,\n",
       " 261,\n",
       " 51196,\n",
       " 306,\n",
       " 1335,\n",
       " 3435,\n",
       " 13,\n",
       " 3627,\n",
       " 11199,\n",
       " 480,\n",
       " 673,\n",
       " 6541,\n",
       " 316,\n",
       " 2107,\n",
       " 483,\n",
       " 480,\n",
       " 2236,\n",
       " 480,\n",
       " 673,\n",
       " 27261,\n",
       " 13,\n",
       " 72406,\n",
       " 7201,\n",
       " 316,\n",
       " 5143,\n",
       " 290,\n",
       " 51196,\n",
       " 483,\n",
       " 1335,\n",
       " 3317,\n",
       " 11,\n",
       " 813,\n",
       " 1770,\n",
       " 2023,\n",
       " 63202,\n",
       " 261,\n",
       " 5057,\n",
       " 402,\n",
       " 1335,\n",
       " 27628,\n",
       " 364,\n",
       " 43,\n",
       " 1810,\n",
       " 5981,\n",
       " 316,\n",
       " 1335,\n",
       " 3317,\n",
       " 326,\n",
       " 2059,\n",
       " 11,\n",
       " 392,\n",
       " 66169,\n",
       " 11,\n",
       " 357,\n",
       " 2491,\n",
       " 495,\n",
       " 51196,\n",
       " 13,\n",
       " 4101,\n",
       " 481,\n",
       " 5143,\n",
       " 480,\n",
       " 483,\n",
       " 668,\n",
       " 326,\n",
       " 63202,\n",
       " 922,\n",
       " 27628,\n",
       " 16842,\n",
       " 6526,\n",
       " 3317,\n",
       " 52846,\n",
       " 326,\n",
       " 2059,\n",
       " 11,\n",
       " 392,\n",
       " 13022,\n",
       " 11,\n",
       " 72406,\n",
       " 11,\n",
       " 581,\n",
       " 665,\n",
       " 5143,\n",
       " 290,\n",
       " 51196,\n",
       " 326,\n",
       " 9295,\n",
       " 634,\n",
       " 27628,\n",
       " 6635,\n",
       " 108008,\n",
       " 11,\n",
       " 1023,\n",
       " 9599,\n",
       " 290,\n",
       " 51196,\n",
       " 326,\n",
       " 458,\n",
       " 23307,\n",
       " 290,\n",
       " 5057,\n",
       " 402,\n",
       " 72406,\n",
       " 885,\n",
       " 27628,\n",
       " 13,\n",
       " 1225,\n",
       " 673,\n",
       " 625,\n",
       " 6541,\n",
       " 395,\n",
       " 1373,\n",
       " 2236,\n",
       " 1023,\n",
       " 1504,\n",
       " 13110,\n",
       " 326,\n",
       " 12137,\n",
       " 2454,\n",
       " 1273,\n",
       " 13,\n",
       " 6311,\n",
       " 1023,\n",
       " 11684,\n",
       " 11,\n",
       " 72406,\n",
       " 104101,\n",
       " 1335,\n",
       " 3317,\n",
       " 395,\n",
       " 13110,\n",
       " 290,\n",
       " 51196,\n",
       " 326,\n",
       " 56239,\n",
       " 1335,\n",
       " 27628,\n",
       " 13,\n",
       " 3164,\n",
       " 2973,\n",
       " 9879,\n",
       " 7150,\n",
       " 2236,\n",
       " 1023,\n",
       " 1458,\n",
       " 9599,\n",
       " 326,\n",
       " 9183,\n",
       " 4717,\n",
       " 13]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dli = iter(dataloader)\n",
    "batch = next(dli)\n",
    "text = batch[\"text\"][0]\n",
    "enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6655f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = enc.n_vocab\n",
    "d_model = 768\n",
    "context_len = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af0b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # token embeddings (vocab size by dimensions)\n",
    "    self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "    # position embeddings (context by dimensions)\n",
    "    self.pos_embed = nn.Embedding(context_len, d_model)\n",
    "    # transformer blocks\n",
    "    # layer norm\n",
    "    # linear\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22726d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # multiple heads\n",
    "\n",
    "    # attention weights:\n",
    "      # qk layers\n",
    "      # matmul qk\n",
    "      # casual mask\n",
    "      # softmax\n",
    "      # dropout\n",
    "\n",
    "    # v layer\n",
    "    # matmul with attention wiehgts\n",
    "    # linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45289be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(),\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.attention = nn.Sequential(\n",
    "      nn.LayerNorm(d_model),\n",
    "      Attention,\n",
    "      nn.Dropout\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.stack(x)\n",
    "    x = F.layer_norm(x)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
